{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lazuardialmuzaki/eda-and-time-series-forecasting-retail-store?scriptVersionId=142689205\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Exploratory Data Analysis (with Choropleth Map) and Sales Time Series Forecasting of SuperStore Retail ","metadata":{}},{"cell_type":"markdown","source":"## 1. Load Data\n\nThe Data used is about Superstore Sales Dataset during 2015 through 2018. Superstore is one of the most popular retail store based in United States of America.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nraw_data = pd.read_csv('/kaggle/input/superstore-sales-dataset/store.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:21:10.840098Z","iopub.execute_input":"2023-09-12T00:21:10.840665Z","iopub.status.idle":"2023-09-12T00:21:10.907288Z","shell.execute_reply.started":"2023-09-12T00:21:10.840615Z","shell.execute_reply":"2023-09-12T00:21:10.90652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:21:11.1671Z","iopub.execute_input":"2023-09-12T00:21:11.168126Z","iopub.status.idle":"2023-09-12T00:21:11.200082Z","shell.execute_reply.started":"2023-09-12T00:21:11.168082Z","shell.execute_reply":"2023-09-12T00:21:11.198674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Checking Missing Values\nChecking missing values are needed so the data used will not be disrupted by row that has null values on column we are interested to analyze","metadata":{}},{"cell_type":"code","source":"raw_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:21:12.460263Z","iopub.execute_input":"2023-09-12T00:21:12.464368Z","iopub.status.idle":"2023-09-12T00:21:12.484059Z","shell.execute_reply.started":"2023-09-12T00:21:12.464299Z","shell.execute_reply":"2023-09-12T00:21:12.483034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the null values only exist in Postal Code which is a column we are not important to our analysis, it will be fine to let the missing values as it be. ","metadata":{}},{"cell_type":"code","source":"# Dropping the column 'Row ID', as it does not help much in the process of data analysis of the dataset.\nraw_data.drop('Row ID',axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:21:13.300429Z","iopub.execute_input":"2023-09-12T00:21:13.30083Z","iopub.status.idle":"2023-09-12T00:21:13.31411Z","shell.execute_reply.started":"2023-09-12T00:21:13.300798Z","shell.execute_reply":"2023-09-12T00:21:13.312875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Extracting Date Information\n\nDate information are essential as our analysis mainly will discover insight regarding time and location.\n\n**Converting Date column type**\n","metadata":{}},{"cell_type":"code","source":"raw_data['Order Date'] = pd.to_datetime(raw_data['Order Date'], format='%d/%m/%Y')  #converting the data type of 'Order Date' column to date time format\nraw_data['Ship Date'] = pd.to_datetime(raw_data['Ship Date'], format='%d/%m/%Y')  #converting the data type of 'Ship Date' column to date time format\nraw_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:21:13.890971Z","iopub.execute_input":"2023-09-12T00:21:13.891835Z","iopub.status.idle":"2023-09-12T00:21:13.945296Z","shell.execute_reply.started":"2023-09-12T00:21:13.89179Z","shell.execute_reply":"2023-09-12T00:21:13.942949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Assign new columns to store Year, Month, and Day information**","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nraw_data['Order Year'] = pd.to_datetime(raw_data['Order Date']).dt.year #store year information\n\nraw_data['Order Month'] = pd.to_datetime(raw_data['Order Date']).dt.month #store month numerical index information\nraw_data['Order MonthName'] = pd.to_datetime(raw_data['Order Date']).dt.month_name() # store month name information\n\nraw_data['Order Day'] = pd.to_datetime(raw_data['Order Date']).dt.day # store day numerical index information\nraw_data['Order DayName'] = pd.to_datetime(raw_data['Order Date']).dt.day_name() # store day name information","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:21:14.405731Z","iopub.execute_input":"2023-09-12T00:21:14.406677Z","iopub.status.idle":"2023-09-12T00:21:14.482877Z","shell.execute_reply.started":"2023-09-12T00:21:14.406638Z","shell.execute_reply":"2023-09-12T00:21:14.481826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:21:14.796262Z","iopub.execute_input":"2023-09-12T00:21:14.796691Z","iopub.status.idle":"2023-09-12T00:21:14.825926Z","shell.execute_reply.started":"2023-09-12T00:21:14.796646Z","shell.execute_reply":"2023-09-12T00:21:14.823576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the DataFrame, it can be seen now several new columns have been added","metadata":{}},{"cell_type":"markdown","source":"## 4. Exploratory Data Analysis\n\nExplorator Data Analysis (EDA) is used to analyze, investigate and summarize main characteristics of datasets through visualization methods. On this ocassion, EDA is specifically used to see if the sales by Superstore may or may not follow some **seasonal trends**.\n\nBased on the columns/features that are given from the DataFrame, it is decided that some EDA specifically designed to see the relationship between accumulation of sales with certain variable which are:\n\n- Sales over Time (Year, Month, Day)\n- Sales over Category (Goods category, Region, State, City)","metadata":{}},{"cell_type":"markdown","source":"### 4.1 EDA: Using Line Chart to Examine Sales over Time","metadata":{}},{"cell_type":"markdown","source":"Sales over time examined through the accumulation of sales over 4 years (2015-2018) vs year, month and day.","metadata":{}},{"cell_type":"code","source":"# Self-defined function is being introduced in order to ease the process of creating line chart\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef linesales(df, category, sort_by, title=None):\n    sns.lineplot(data=df.groupby([category]).sum(\"Sales\").sort_values(sort_by, ascending=True)['Sales'])\n    plt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large')\n    plt.title(label=title, fontdict={'fontsize': 15})","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:33:14.843157Z","iopub.execute_input":"2023-09-12T00:33:14.843598Z","iopub.status.idle":"2023-09-12T00:33:14.851285Z","shell.execute_reply.started":"2023-09-12T00:33:14.843564Z","shell.execute_reply":"2023-09-12T00:33:14.849832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting Sales over Years, Months, and Days**","metadata":{}},{"cell_type":"code","source":"# Subplots summoned as foundation for later charts\n\nplt.subplots(3,1, figsize=(8,14))\n\n\nplt.subplot(3,1,1)\nlinesales(raw_data, 'Order Year', 'Order Year', 'Total Sales over Years')\nplt.tight_layout()\n\nplt.subplot(3,1,2)\nlinesales(raw_data, 'Order MonthName', 'Order Month', \"Average Sales per Month\")\nplt.tight_layout()\n\nplt.subplot(3,1,3)\nlinesales(raw_data, 'Order DayName', 'Order Day', \"Average Sales per Day\")\nplt.tight_layout()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:33:15.682293Z","iopub.execute_input":"2023-09-12T00:33:15.682718Z","iopub.status.idle":"2023-09-12T00:33:16.77387Z","shell.execute_reply.started":"2023-09-12T00:33:15.682683Z","shell.execute_reply":"2023-09-12T00:33:16.772786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From graph above, it could be seen that:\n\n- First figure of 'Total Sales over Years' depicts  how the last two years have recorded significant steady rise. From around 480,000 sales in 2015, it was dropped slightly to approximately 460,000 a year later. During 2017 and 2018, it shows a promising rise to around 600,000 and 720,000 sales respectively\n\n- Second figure of 'Total Sales per Month' strongly indicates that there is a trend during last four months of the year (September to December) in which the sales rising convincingly\n\n- Third figure of 'Total Sales per Day' shows that two days of weekends have higher sales (Saturday & Sunday) compared to all days of weekdays except Tuesday which may be indicating another kind of trend.\n","metadata":{}},{"cell_type":"markdown","source":"### 4.2 EDA: Using Bar Chart and Map to Describe Sales over Category\n\nSales over category is examined through the accumulation of sales over all years combined vs category of goods, region, city and states.\n\nBefore we begin, total amounts of category level is examined for every category that will be analyzed.","metadata":{}},{"cell_type":"code","source":"print(\"Category of Goods: \"+str(len(raw_data['Category'].unique())))\nprint(\"Region: \"+str(len(raw_data['Region'].unique())))\nprint(\"State: \"+str(len(raw_data['State'].unique())))\nprint(\"City: \"+str(len(raw_data['City'].unique())))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:33:26.221872Z","iopub.execute_input":"2023-09-12T00:33:26.22229Z","iopub.status.idle":"2023-09-12T00:33:26.232418Z","shell.execute_reply.started":"2023-09-12T00:33:26.222255Z","shell.execute_reply":"2023-09-12T00:33:26.231473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this result, it will be convenient to explain using bar chart for 'Category of Goods' and 'Region' as they have small level of category.\n\nFor 'State', it might be best to take benefit of geographic map of United States to show the sales accumulation comparison.\n\nHowever for 'City', there are so many class. For that, bar chart is the one which is used but with modification that only top 10 selling city will be included in the chart.","metadata":{}},{"cell_type":"markdown","source":"#### Plotting Sales over Category of Goods and Region using Bar Chart","metadata":{}},{"cell_type":"code","source":"# self-defined function is developed to ease the creation of bar chart\n\ndef barsales(df, category, title=None):\n    data = df.groupby([category])['Sales'].sum().reset_index().sort_values('Sales', ascending=True)\n    sns.barplot(data= data, y=data[category], x=data['Sales'])\n    plt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large')\n    plt.title(label=title)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:33:27.150234Z","iopub.execute_input":"2023-09-12T00:33:27.151242Z","iopub.status.idle":"2023-09-12T00:33:27.157385Z","shell.execute_reply.started":"2023-09-12T00:33:27.151206Z","shell.execute_reply":"2023-09-12T00:33:27.156224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(2,1, figsize=(8,10))\n\n\nplt.subplot(2,1,1)\nbarsales(raw_data, 'Category', 'Overall Sales over Category of Goods')\nplt.tight_layout()\n\nplt.subplot(2,1,2)\nbarsales(raw_data, 'Region', 'Overall Sales over Region') \nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:33:27.374616Z","iopub.execute_input":"2023-09-12T00:33:27.374997Z","iopub.status.idle":"2023-09-12T00:33:27.899083Z","shell.execute_reply.started":"2023-09-12T00:33:27.374968Z","shell.execute_reply":"2023-09-12T00:33:27.897904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From result above, it is apparent that for 'Category of Goods', 'Technology' is one best-selling type of product. but not necessarily significant compared to the other two type of product. \n\nFor 'Region', West region has gotten the most sales followed by East, Central, and South respectively.","metadata":{}},{"cell_type":"markdown","source":"#### Plotting Sales over States","metadata":{}},{"cell_type":"markdown","source":"As mentioned earlier, different approach is being used to describe Sales over State which is by using geographical map.\nIn doing so, additional column information is being integrated and designated dataframe is being prepared ","metadata":{}},{"cell_type":"code","source":"# Creating list for name of State and State Code which later used as new column of designated dataframe\n\nstate = ['Alabama', 'Arizona' ,'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', \n         'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n         'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana','Nebraska', 'Nevada', 'New Hampshire',\n         'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n         'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n         'West Virginia', 'Wisconsin','Wyoming']\nstate_code = ['AL','AZ','AR','CA','CO','CT','DE','FL','GA','HI','ID','IL','IN','IA','KS','KY','LA','ME','MD','MA',\n              'MI','MN','MS','MO','MT','NE','NV','NH','NJ','NM','NY','NC','ND','OH','OK','OR','PA','RI','SC','SD','TN',\n              'TX','UT','VT','VA','WA','WV','WI','WY']","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:33:32.993585Z","iopub.execute_input":"2023-09-12T00:33:32.993994Z","iopub.status.idle":"2023-09-12T00:33:33.002196Z","shell.execute_reply.started":"2023-09-12T00:33:32.993957Z","shell.execute_reply":"2023-09-12T00:33:33.000826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a designated dataframe for later map plotting\n\nstate_df = pd.DataFrame(state, state_code) # Create a dataframe\nstate_df.reset_index(level=0, inplace=True)\nstate_df.columns = ['State Code','State']\nsales = raw_data.groupby([\"State\"]).sum(\"Sales\").sort_values(\"Sales\", ascending=False)\nsales.reset_index(level=0, inplace=True)\nsales.drop(labels=['Postal Code','Order Year','Order Month','Order Day'],axis=1, inplace = True)\nsales= sales.sort_values('State', ascending=True)\nsales.reset_index(inplace = True)\nsales.drop(labels='index',axis=1,inplace = True)\nsales.insert(1, 'State Code', state_df['State Code'])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:29:41.073928Z","iopub.execute_input":"2023-09-12T00:29:41.074639Z","iopub.status.idle":"2023-09-12T00:29:41.093773Z","shell.execute_reply.started":"2023-09-12T00:29:41.074604Z","shell.execute_reply":"2023-09-12T00:29:41.092561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:29:43.921022Z","iopub.execute_input":"2023-09-12T00:29:43.921427Z","iopub.status.idle":"2023-09-12T00:29:43.937391Z","shell.execute_reply.started":"2023-09-12T00:29:43.921392Z","shell.execute_reply":"2023-09-12T00:29:43.93651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the DataFrame is ready, now we start to develop the function to plot a map based on this DataFrame.\n\nThe map used here is **Choropleth Map**, a type of statistical thematic map that uses pseudocolor corresponding with an aggregate summary of a geographic characteristic within spatial enumeration units. For this case, it will be about Sales accumulation of a state.","metadata":{}},{"cell_type":"code","source":"# Implementing the designated DataFrame to Choropleth Map\n\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, plot_mpl\n\ndef plotmap(category, title):\n    sales['text'] = sales[category]\n    fig = go.Figure(data=go.Choropleth(\n        locations=sales['State Code'], # Spatial coordinates\n        text=sales['text'],\n        z = sales['Sales'].astype(float), # Data to be color-coded\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        colorscale = 'Greens',\n        colorbar_title = \"Sales\",\n    ))\n    fig.update_layout(\n        title_text = title,\n        geo_scope='usa', # limite map scope to USA\n    )\n    fig.show();\n    \nplotmap('State','Sales over States')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:29:48.149212Z","iopub.execute_input":"2023-09-12T00:29:48.149584Z","iopub.status.idle":"2023-09-12T00:29:48.626754Z","shell.execute_reply.started":"2023-09-12T00:29:48.149554Z","shell.execute_reply":"2023-09-12T00:29:48.625579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Choropleth Map above explains how different gradient of green color exemplify different magnitude of total sales. California has the most sales over all years with around 446,000 sales, followed by New York and Texas (around 306,000 and 169,000 respectively)","metadata":{}},{"cell_type":"markdown","source":"#### Plotting Sales over City","metadata":{}},{"cell_type":"markdown","source":"For City category, Top 10 Cities are selected and then will feature on bar chart.","metadata":{}},{"cell_type":"code","source":"city_df=raw_data.groupby(\"City\").sum(\"Sales\").sort_values(\"Sales\", ascending = False)\ncity_df = city_df[[\"Sales\"]]\ncity_df.reset_index(inplace=True)\n\n\nmerged_city_state = pd.merge(city_df, raw_data[['City','State']], how=\"inner\", on=[\"City\"])\nmerged_city_state = merged_city_state.drop_duplicates(subset='City')\nmerged_city_state['City,State'] = merged_city_state['City'] + \", \" + merged_city_state['State']\n\nsns.barplot(x = \"Sales\" , y = \"City,State\", data = merged_city_state[:10],edgecolor = \"white\", palette = \"pastel\")\nplt.xlabel(\"Sales\", size = 20,color='black')\n\nplt.ylabel(\"City, State\", size = 20,color='black')\nplt.title(\"Top_10_Cities in Sales\", size = 20, pad = 20,color='black')\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-12T00:30:08.116342Z","iopub.execute_input":"2023-09-12T00:30:08.11676Z","iopub.status.idle":"2023-09-12T00:30:08.802702Z","shell.execute_reply.started":"2023-09-12T00:30:08.116726Z","shell.execute_reply":"2023-09-12T00:30:08.801591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From result above, we have Top 10 Cities in Sales which stakeholder might want to focus more on. New York City in New York state is shown dominating with total sales of approximately 250,000 sales. ","metadata":{}},{"cell_type":"markdown","source":"## 5. Time Series Analysis\nTime series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. In this case, we want to perform time series analysis in order to be able to predict the sales performance to avoid 'out of stock' situation in future","metadata":{}},{"cell_type":"code","source":"#import the necessary Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:14.256907Z","iopub.execute_input":"2023-09-12T00:30:14.257378Z","iopub.status.idle":"2023-09-12T00:30:14.263312Z","shell.execute_reply.started":"2023-09-12T00:30:14.257332Z","shell.execute_reply":"2023-09-12T00:30:14.261996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1 Dataframe Preparation","metadata":{}},{"cell_type":"code","source":"#sorting data by order date\nraw_data.sort_values(by=['Order Date'], inplace=True, ascending=True) #Sorting data by  ascending order of the coloumn values 'Order Date'\nraw_data.set_index(\"Order Date\", inplace = True) #Setting 'Order Date' as index of the dataframe 'df' for ease of Time Series Analysis","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:15.810948Z","iopub.execute_input":"2023-09-12T00:30:15.811315Z","iopub.status.idle":"2023-09-12T00:30:15.824038Z","shell.execute_reply.started":"2023-09-12T00:30:15.811285Z","shell.execute_reply":"2023-09-12T00:30:15.822915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To forecast sales seven days later of the order date, let us create a new dataframe with only the target column i.e,\n# the 'Sales' column and 'Order Date' as the index\n\nnew_data = pd.DataFrame(raw_data['Sales'])\nnew_data","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:17.656136Z","iopub.execute_input":"2023-09-12T00:30:17.657059Z","iopub.status.idle":"2023-09-12T00:30:17.672312Z","shell.execute_reply.started":"2023-09-12T00:30:17.657015Z","shell.execute_reply":"2023-09-12T00:30:17.670885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting the data to understand the sales distribution from the year 2015-2018\nnew_data.plot(figsize=(20, 12))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:19.408902Z","iopub.execute_input":"2023-09-12T00:30:19.409301Z","iopub.status.idle":"2023-09-12T00:30:19.939231Z","shell.execute_reply.started":"2023-09-12T00:30:19.409268Z","shell.execute_reply":"2023-09-12T00:30:19.938086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Stationarity Tests\nA series is said to be stationary when its mean and variance do not change over time. From the above distribution of the sales it is not clear whether the sales distribution is stationary or not. Let us perform some stationarity tests to check whether the time series is stationary or not.","metadata":{}},{"cell_type":"code","source":"#Checkting for Stationarity\nnew_data =  pd.DataFrame(new_data['Sales'].resample('D').mean())\nnew_data = new_data.interpolate(method='linear') #The interpolate() function is used to interpolate values according to\n#different methods. It ignore the index and treats the values as equally spaced.","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:21.421148Z","iopub.execute_input":"2023-09-12T00:30:21.421574Z","iopub.status.idle":"2023-09-12T00:30:21.441721Z","shell.execute_reply.started":"2023-09-12T00:30:21.421537Z","shell.execute_reply":"2023-09-12T00:30:21.44071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To check for stationarity by comparing the change in mean and variance over time, let us split the data into train, test and validate.\ntrain, test, validate = np.split(new_data['Sales'].sample(frac=1), [int(.6*len(new_data['Sales'])),int(.8*len(new_data['Sales']))])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:21.724948Z","iopub.execute_input":"2023-09-12T00:30:21.725324Z","iopub.status.idle":"2023-09-12T00:30:21.731795Z","shell.execute_reply.started":"2023-09-12T00:30:21.725294Z","shell.execute_reply":"2023-09-12T00:30:21.731017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train Dataset')\nprint(train)\nprint('Test Dataset')\nprint(test)\nprint('Validate Dataset')\nprint(validate)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:22.143623Z","iopub.execute_input":"2023-09-12T00:30:22.144308Z","iopub.status.idle":"2023-09-12T00:30:22.155695Z","shell.execute_reply.started":"2023-09-12T00:30:22.144263Z","shell.execute_reply":"2023-09-12T00:30:22.15445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean1, mean2, mean3 = train.mean(), test.mean(), validate.mean() #taking mean of train, test and validate data\nvar1, var2, var3 = train.var(), test.var(), validate.var() #taking variance of train, test and validate data\n\nprint('Mean:')\nprint(mean1, mean2, mean3)\nprint('Variance:')\nprint(var1, var2, var3)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:22.539015Z","iopub.execute_input":"2023-09-12T00:30:22.53971Z","iopub.status.idle":"2023-09-12T00:30:22.547302Z","shell.execute_reply.started":"2023-09-12T00:30:22.539672Z","shell.execute_reply":"2023-09-12T00:30:22.546198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above values of mean and variance, it can be inferred that their is not much difference in the three values of mean and variance, indicating that the series is stationary. However, to verify our observations, let us perform a standard stationarity test, called Augmented Dicky Fuller test.","metadata":{}},{"cell_type":"markdown","source":"### 5.3 Augmented Dicky Fuller test\nThe Augmented Dickey-Fuller test is a type of statistical test alsocalled a unit root test.The base of unit root test is that it helps in determining how strongly a time series is defined by a trend.\n\nThe null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary. The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.\n\n1.Null Hypothesis(H0): Time series is not stationary\n2.Alternate Hypothesis (H1): Time series is stationary\n\nThis result is interpreted using the p-value from the test.\n\n1.p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n2.p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.","metadata":{}},{"cell_type":"code","source":"# Method 2\n# Augmented Dicky Fuller Test\n\nfrom statsmodels.tsa.stattools import adfuller #importing adfuller tool from statsmodels\n#statsmodels provide adfuller() fucntion to implement stationarity test of a time series\n\nadf = adfuller(new_data)\n\nprint(adf)\nprint('\\nADF = ', str(adf[0])) #more towards negative value the better\nprint('\\np-value = ', str(adf[1]))\nprint('\\nCritical Values: ')\n\nfor key, val in adf[4].items(): #for loop to print the p-value (1%, 5% and 10%) and their respective values\n    print(key,':',val)\n\n\n    if adf[0] < val:\n        print('Null Hypothesis Rejected. Time Series is Stationary')\n    else:\n        print('Null Hypothesis Accepted. Time Series is not Stationary')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:24.848983Z","iopub.execute_input":"2023-09-12T00:30:24.849613Z","iopub.status.idle":"2023-09-12T00:30:25.143724Z","shell.execute_reply.started":"2023-09-12T00:30:24.849582Z","shell.execute_reply":"2023-09-12T00:30:25.140184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pylab import rcParams\nrcParams['figure.figsize'] = 30, 20\n\nimport statsmodels.api as sm\ndecomposition = sm.tsa.seasonal_decompose(new_data, model='additive') #function used to decompose Time Series Data into Trend and Seasonality\nfig = decomposition.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:26.376782Z","iopub.execute_input":"2023-09-12T00:30:26.37715Z","iopub.status.idle":"2023-09-12T00:30:30.291086Z","shell.execute_reply.started":"2023-09-12T00:30:26.377122Z","shell.execute_reply":"2023-09-12T00:30:30.289955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we know our time series is data is stationary. Let us begin with model training for forecasting the sales. We have chosen SARIMA model to forecast the sales.\n\nSeasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that supports univariate time series data with a seasonal component.\n\nSARIMA requires selecting hyperparameters for both the trend and seasonal elements of the series.\n\nTrend Elements There are three trend elements that require configuration.\np: Trend autoregression order. d: Trend difference order. q: Trend moving average order.\n\nSeasonal Elements There are four seasonal elements:\nP: Seasonal autoregressive order. D: Seasonal difference order. Q: Seasonal moving average order. m: The number of time steps for a single seasonal period.\n\nThe notation for a SARIMA model is specified as: SARIMA(p,d,q)(P,D,Q)m","metadata":{}},{"cell_type":"markdown","source":"### 5.4 SARIMA model","metadata":{}},{"cell_type":"code","source":"import itertools\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq_comb = [(i[0], i[1], i[2], 12) for i in list(itertools.product(p, d, q))] #for loop for creating combinations of seasonal parameters of SARIMA\nprint('Examples of parameter combinations for Seasonal ARIMA:')\nprint('SARIMA: {} x {}'.format(pdq[1], seasonal_pdq_comb[1]))\nprint('SARIMA: {} x {}'.format(pdq[1], seasonal_pdq_comb[2]))\nprint('SARIMA: {} x {}'.format(pdq[2], seasonal_pdq_comb[3]))\nprint('SARIMA: {} x {}'.format(pdq[2], seasonal_pdq_comb[4]))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:30.293255Z","iopub.execute_input":"2023-09-12T00:30:30.293881Z","iopub.status.idle":"2023-09-12T00:30:30.303572Z","shell.execute_reply.started":"2023-09-12T00:30:30.293844Z","shell.execute_reply":"2023-09-12T00:30:30.302468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for parameters in pdq: #for loop for determining the best combination of seasonal parameters for SARIMA\n    for seasonal_param in seasonal_pdq_comb:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(new_data,\n                                            order=parameters,\n                                            seasonal_param_order=seasonal_param,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False) #determines the AIC value of the model**\n            results = mod.fit(disp=0)\n            print('SARIMA{}x{}12 - AIC:{}'.format(parameters, seasonal_param, results.aic))\n        except:\n            continue","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:30.305081Z","iopub.execute_input":"2023-09-12T00:30:30.305552Z","iopub.status.idle":"2023-09-12T00:30:40.621196Z","shell.execute_reply.started":"2023-09-12T00:30:30.305492Z","shell.execute_reply":"2023-09-12T00:30:40.619899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The Akaike information criterion (AIC) is an estimator of out-of-sample prediction error and thereby relative**\nquality of statistical models for a given set of data. AIC estimates the relative amount of information lost\nby a given model. The less information a model loses, the higher the quality of that model.","metadata":{}},{"cell_type":"code","source":"# After choosing the combination of seasonal parameters with least AIC value, let us train the SARIMA model\nmod = sm.tsa.statespace.SARIMAX(new_data,\n                                order=(1, 1, 1),\n                                seasonal_order=(1, 1, 1, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False) #model defintion\nresults = mod.fit(disp=0) #model fitting\nprint(results.summary().tables[1]) # displaying the result","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:40.623768Z","iopub.execute_input":"2023-09-12T00:30:40.624224Z","iopub.status.idle":"2023-09-12T00:30:48.461601Z","shell.execute_reply.started":"2023-09-12T00:30:40.624179Z","shell.execute_reply":"2023-09-12T00:30:48.460567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.plot_diagnostics(figsize=(16, 8))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:48.4647Z","iopub.execute_input":"2023-09-12T00:30:48.465069Z","iopub.status.idle":"2023-09-12T00:30:49.575763Z","shell.execute_reply.started":"2023-09-12T00:30:48.465038Z","shell.execute_reply":"2023-09-12T00:30:49.574537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Produces a plot grid of:\n1. Standardized residuals over time\n2. Histogram plus estimated density of standardized residulas and along with a Normal(0,1) density plotted for reference.\n3. Normal Q-Q plot, with Normal reference line\n4. Correlogram.","metadata":{}},{"cell_type":"code","source":"pred = results.get_prediction(start=pd.to_datetime('2015-01-03'), dynamic=False) # variable to display plot for predicted values\npred_val = pred.conf_int()\nax = new_data['2014':].plot(label='observed') # displays plot for original values\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(40, 26)) # displays plot for predicted values\nax.fill_between(pred_val.index,\n                pred_val.iloc[:, 0],\n                pred_val.iloc[:, 1], color='k', alpha=.2)\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:52.853101Z","iopub.execute_input":"2023-09-12T00:30:52.853511Z","iopub.status.idle":"2023-09-12T00:30:54.139517Z","shell.execute_reply.started":"2023-09-12T00:30:52.853466Z","shell.execute_reply":"2023-09-12T00:30:54.138535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_forecasted = pred.predicted_mean\ny_truth = new_data['Sales']\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nmse = mean_squared_error(y_forecasted, y_truth)\nrmse = sqrt(mse)\nprint('The Mean Squared Error of the forecasts is {}'.format(round(rmse, 2))) # displays the root mean squared error of the forecast with rounding it up to 2 decimals","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:54.140997Z","iopub.execute_input":"2023-09-12T00:30:54.141411Z","iopub.status.idle":"2023-09-12T00:30:54.293486Z","shell.execute_reply.started":"2023-09-12T00:30:54.141383Z","shell.execute_reply":"2023-09-12T00:30:54.292185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.5 Out of Sample Forecast\nTo forecast sales values after some time period of the given data. In our case, we have to forecast sales with time period of 7 days.","metadata":{}},{"cell_type":"code","source":"forecast_data = results.forecast(steps=30) # making a forecast of a month later of the last date in the 'Order Date' column\nprint(forecast_data.astype('int')) #displays the sales forecast as type integer\ndef forecast(data):\n    data.plot(figsize=(20, 12))\n    plt.title('Figure 2. One Month Forecast of Sales Performance')\n    plt.show()\nforecast(forecast_data)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:30:56.247318Z","iopub.execute_input":"2023-09-12T00:30:56.248107Z","iopub.status.idle":"2023-09-12T00:30:56.701642Z","shell.execute_reply.started":"2023-09-12T00:30:56.248069Z","shell.execute_reply":"2023-09-12T00:30:56.700785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Study Case Summary","metadata":{}},{"cell_type":"markdown","source":"Superstore is one of popular retail store that operates across United States. But recently, the stakeholders are informed that in overall, some stores have been encountering case of 'out of stock'. The stakeholders then asked the data analytics department to examine the 4-year sales historical data to gain more detailed description of sales performance. \n\nExploratory Data Analysis (EDA) then is implemented to show how sales changes over the years, months and days indicating the trends that might have been existed incorporating customer buying behaviour. Sales performance evaluation over different region, states and city are also performed utilizing charts and map to obtain clear picture to distinguish best-selling area that might become a focus for stakeholder to improve. First figure below shows how sales performance differs across the states.\n\nFurthermore, a time series analysis is also conducted based on the SARIMA model to predict the sales performance for upcoming period to avoid ‘out of stock’ situation. Example on second figure below shows an overall sales prediction one month after the last time-node of the dataset. It is found that the sales volume has several maximum values, but as time goes by, it gradually tends to be stable. According to the prediction results of the model, the daily sales volume of the store in overall will be stable at around 230 units.\n\nThe sales prediction can be modified to specific criteria depends on needs (for example: monthly prediction for New York City), but the challenge will be about preparing the data prior to the time series analysis.\n","metadata":{}},{"cell_type":"code","source":"plotmap('State', 'Figure 1. Sales over States')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:31:01.267955Z","iopub.execute_input":"2023-09-12T00:31:01.268378Z","iopub.status.idle":"2023-09-12T00:31:01.284444Z","shell.execute_reply.started":"2023-09-12T00:31:01.268344Z","shell.execute_reply":"2023-09-12T00:31:01.283567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast(forecast_data)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T00:31:02.205353Z","iopub.execute_input":"2023-09-12T00:31:02.206103Z","iopub.status.idle":"2023-09-12T00:31:02.609759Z","shell.execute_reply.started":"2023-09-12T00:31:02.206055Z","shell.execute_reply":"2023-09-12T00:31:02.608477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}